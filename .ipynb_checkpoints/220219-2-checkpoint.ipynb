{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab35e4cf",
   "metadata": {},
   "source": [
    "# 정규표현식\n",
    "## 정규식을 이용한 문자열 검색 - match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7c08b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='python'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile('[a-z]+')\n",
    "m = p.match(\"python\")\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9850fbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "m = p.match(\"3 python\")\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e69bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found : string\n"
     ]
    }
   ],
   "source": [
    "#p = re.compile(\"정규표현식\")\n",
    "p = re.compile('[a-z]+')\n",
    "m = p.match('string goes here')\n",
    "if m:\n",
    "    print(\"Match found :\", m.group())\n",
    "else:\n",
    "    print(\"No match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84a2c9",
   "metadata": {},
   "source": [
    "## 정규식을 이용한 문자열 검색 - search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ff076b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='python'>\n"
     ]
    }
   ],
   "source": [
    "m = p.search(\"python\")\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc02eee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(2, 8), match='python'>\n"
     ]
    }
   ],
   "source": [
    "m = p.search(\"3 python\")\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee6d741",
   "metadata": {},
   "source": [
    "## 정규식을 이용한 문자열 검색 - findall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2f04812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'is', 'too', 'short']\n"
     ]
    }
   ],
   "source": [
    "result = p.findall(\"life is too short\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92227237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'is', 'too', 'hort']\n"
     ]
    }
   ],
   "source": [
    "result = p.findall(\"life is 3 too Short\")    # 숫자, 대문자는 안 받는다\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbac402",
   "metadata": {},
   "source": [
    "## 정규식을 이용한 문자열 검색 - finditer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5512a4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<callable_iterator object at 0x00000217949A8940>\n",
      "<re.Match object; span=(0, 4), match='life'>\n",
      "<re.Match object; span=(5, 7), match='is'>\n",
      "<re.Match object; span=(8, 11), match='too'>\n",
      "<re.Match object; span=(12, 17), match='short'>\n"
     ]
    }
   ],
   "source": [
    "result = p.finditer(\"life is too short\")\n",
    "print(result)\n",
    "\n",
    "for r in result: print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f9cfb1",
   "metadata": {},
   "source": [
    "## match 객체의 메서드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "368b86ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = p.match(\"python\")\n",
    "m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec73e3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.start()    # span의 첫 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "312b9923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.end()    # span의 마지막 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c56a5fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 6)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f06f6be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = p.search(\"3 python\")\n",
    "m.group()    # 처음 나온 매치 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1814580e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "112fbf5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d73ce430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64711b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile('[a-z]+')\n",
    "m = p.match(\"python\")\n",
    "m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a9cf4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = re.match('[a-z]+', 'python')    # 위의 것과 동일한 결과\n",
    "m.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3e851",
   "metadata": {},
   "source": [
    "## 컴파일 옵션 - DOTALL, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e940155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile('a.b')\n",
    "m = p.match('a\\nb')\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54a9bde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='a\\nb'>\n"
     ]
    }
   ],
   "source": [
    "p = re.compile('a.b', re.DOTALL)    # \\n도 .으로 받는다\n",
    "m = p.match('a\\nb')\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "775ac8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='a\\nb'>\n"
     ]
    }
   ],
   "source": [
    "p = re.compile('a.b', re.S)\n",
    "m = p.match('a\\nb')\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a24ee80",
   "metadata": {},
   "source": [
    "## 컴파일 옵션 - IGNORECASE, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a881eb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='python'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile('[a-z]+', re.I)    # 대소문자 구분 없이 쓴다\n",
    "p.match('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3f74cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='Python'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.match('Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ec7a4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='PYTHON'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.match(\"PYTHON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4189c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='pyThon'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile('[a-z]+', re.IGNORECASE)\n",
    "p.match('pyThon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e99ff7",
   "metadata": {},
   "source": [
    "## 컴파일 옵션 - MULTILINE, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc84a66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python one']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile(\"^python\\s\\w+\")    # 문자열이 python으로 시작하며(^) 뒤에 \\n\\t(\\s)같은게 오고 문자(\\w) 오는것\n",
    "# ^ - 문자열이 이걸로 시작할 것  \n",
    "# $ - 문자열이 이걸로 끝날것\n",
    "\n",
    "data = \"\"\"python one\n",
    "life is too short\n",
    "python two\n",
    "you need python\n",
    "python tree\"\"\"\n",
    "\n",
    "print(p.findall(data))    # 전체 문자열이라 맨 처음 python만 나온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb786db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['three']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile(\"three$\", re.MULTILINE)\n",
    "\n",
    "data = \"\"\"python one\n",
    "life is too short\n",
    "python two\n",
    "you need python\n",
    "python three\"\"\"\n",
    "\n",
    "print(p.findall(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d41aaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python one', 'python two', 'python tree']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile(\"^python\\s\\w+\", re.MULTILINE)    # 라인별로 매치 검사\n",
    "\n",
    "data = \"\"\"python one\n",
    "life is too short\n",
    "python two\n",
    "you need python\n",
    "python tree\"\"\"\n",
    "\n",
    "print(p.findall(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5a2fb6",
   "metadata": {},
   "source": [
    "## 표제어 추출(Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "400fb29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "# 단어 원문을 찾는다는 뜻인듯\n",
    "# 단어의 품사 정보를 알아야 정확하게 할 수 있다\n",
    "# am, are, is 의 표제어는 be\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "n = WordNetLemmatizer()\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([n.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06fdce95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('dies', 'v')    # 'v'는 품사가 동사라는 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9004d64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('watched', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2608d80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('has', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9774540",
   "metadata": {},
   "source": [
    "## 어간 추출(Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a03d6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'as', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'thins', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
      "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'as', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thin', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "# 품사 정보가 보존되지 않는다\n",
    "# 단순한 규칙에 의해 이루어짐\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentence = \"\"\"This was not the map we found in Billy Bones's chest, but as accurate copy, complete\n",
    "in all thins--names and heights and soundings--with the single exception of the red crosses and\n",
    "the written notes.\"\"\"\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "print(\"어간 추출 전 :\", tokenized_sentence)\n",
    "print(\"어간 추출 후 :\", [stemmer.stem(word) for word in tokenized_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9075a185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['formalize', 'allowance', 'electricical']\n",
      "어간 추출 후 : ['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "# 포터 알고리즘의 어간 추출\n",
    "# alize -> al\n",
    "# ance -> 제거\n",
    "# ical - ic\n",
    "words = ['formalize', 'allowance', 'electricical']\n",
    "\n",
    "print(\"어간 추출 전 :\", words)\n",
    "print(\"어간 추출 후 :\", [stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce9b7e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "포터 스테머의 어간 추출 후 : ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
      "랭커스터 스테머의 어간 추출 후 : ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "# 어간추출은 표제어 추출보다 빠르다\n",
    "# 영어는 어간 추출로도 괜찮은 결과가 나온다\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print(\"어간 추출 전 :\", words)\n",
    "print(\"포터 스테머의 어간 추출 후 :\", [porter_stemmer.stem(w) for w in words])\n",
    "print(\"랭커스터 스테머의 어간 추출 후 :\", [lancaster_stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82739e66",
   "metadata": {},
   "source": [
    "## 한국어에서의 어간 추출\n",
    "활용 : 용어의 어간과 어미가 있는것  \n",
    "ex) 규칙 활용 : 잡(어간) + 다(어미)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5ebde",
   "metadata": {},
   "source": [
    "## NLTK에서 불용어 제거하기\n",
    "불용어(Stopword) : 자주 등장하지만 해석에 크게 기여하지는 않는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d3ecaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "stop_words_list = stopwords.words('english')    # nltk가 정의한 불용어 리스트 넘김\n",
    "print(\"불용어 개수 :\", len(stop_words_list))\n",
    "print(\"불용어 10개 출력 :\", stop_words_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c27d9974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:    # 불용어가 아닌 단어들만 result에 입력\n",
    "        result.append(word)\n",
    "        \n",
    "print(\"불용어 제거 전 :\", word_tokens)\n",
    "print(\"불용어 제거 후 :\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df76fd80",
   "metadata": {},
   "source": [
    "## 한국어에서 불용어 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2513bb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
      "불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"    # 불용어 정의\n",
    "\n",
    "stop_words = set(stop_words.split(' '))\n",
    "word_tokens = okt.morphs(example)\n",
    "\n",
    "result = [word for word in word_tokens if not word in stop_words]    # 불용어를 제거한 나머지를 result에 입력\n",
    "\n",
    "print(\"불용어 제거 전 :\", word_tokens)\n",
    "print(\"불용어 제거 후 :\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cc3ef3",
   "metadata": {},
   "source": [
    "## 정규 표현식을 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "75d8417e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w]+\")\n",
    "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88ab9e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43da4c5",
   "metadata": {},
   "source": [
    "## 정수 인코딩\n",
    "### 1) dictionary 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cf12c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7a826b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "456a6d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "sentences = sent_tokenize(raw_text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55934b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sentence in sentences:\n",
    "    # 단어 토큰화\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    result = []\n",
    "    for word in tokenized_sentence:\n",
    "        word = word.lower()    # 모든 단어를 소문자화하여 단어의 개수를 줄인다\n",
    "        if word not in stop_words:    # 단어 토큰화 된 결과에 대해서 불용어를 제거한다\n",
    "            if len(word) > 2:    # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거한다\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0\n",
    "                vocab[word] += 1    # {'barber', 1} barber가 나올수록 숫자 증가\n",
    "    preprocessed_sentences.append(result)\n",
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e3d87307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab['barber'])    # 'barber'라는 단어의 빈도수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36faf1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
